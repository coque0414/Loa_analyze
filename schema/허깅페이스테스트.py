from transformers import AutoTokenizer, AutoModel
import torch

# 1. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
model_name = "BM-K/KoSimCSE-roberta-multitask"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# 2. í…ŒìŠ¤íŠ¸ ë¬¸ì¥
sentence = "ë‚™ì› ì½˜í…ì¸ "

# 3. í† í°í™” (ì…ë ¥ í…ì„œë¡œ ë³€í™˜)
inputs = tokenizer(sentence, return_tensors="pt", truncation=True, padding=True)

# 4. ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ hidden states ì–»ê¸°
with torch.no_grad():
    outputs = model(**inputs)
    last_hidden_state = outputs.last_hidden_state  # (batch, seq_len, hidden_size)
    attention_mask = inputs['attention_mask']

# 5. Sentence-Embedding: [CLS] ë²¡í„° or Mean Pooling
# ğŸ‘‰ KoSimCSEëŠ” ì¼ë°˜ì ìœ¼ë¡œ Mean Pooling ì‚¬ìš©
sentence_embedding = (last_hidden_state * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True)

print(sentence_embedding.shape)  # (1, 768)
print(sentence_embedding)
