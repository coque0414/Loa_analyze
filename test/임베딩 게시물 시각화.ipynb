{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제 사용하는 파이썬: c:\\Users\\lee\\.conda\\envs\\loa309\\python.exe\n",
      "PyTorch 버전: 2.7.1+cu126\n",
      "빌드된 CUDA 버전: 12.6\n",
      "CUDA 사용 가능: True\n",
      "NVIDIA GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "import sys, torch\n",
    "print(\"실제 사용하는 파이썬:\", sys.executable)\n",
    "print(\"PyTorch 버전:\", torch.__version__)\n",
    "print(\"빌드된 CUDA 버전:\", torch.version.cuda)\n",
    "print(\"CUDA 사용 가능:\", torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim version: 3.8.3\n",
      "summarize 모듈 OK\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(\"gensim version:\", gensim.__version__)\n",
    "\n",
    "# summarization 모듈 확인\n",
    "from gensim.summarization import summarize\n",
    "print(\"summarize 모듈 OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5246,
     "status": "ok",
     "timestamp": 1749461037895,
     "user": {
      "displayName": "이경민AI융합학과",
      "userId": "11168314652259053945"
     },
     "user_tz": -540
    },
    "id": "rOhmkzcVpOLF",
    "outputId": "a0228fef-00c3-496d-e53a-574152e2fd6b"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install -q pymongo[srv] sentence-transformers tqdm plotly langchain openai accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "error",
     "timestamp": 1749461317547,
     "user": {
      "displayName": "이경민AI융합학과",
      "userId": "11168314652259053945"
     },
     "user_tz": -540
    },
    "id": "hhHFb7jUpf8v",
    "outputId": "f57ccfc3-4a20-4cd9-9ccd-2c48182b5bb7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\lee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ✅ 전체 시계열 기반 LSTM 학습 구조 (연속 임베딩 시퀀스 입력)\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from transformers import pipeline, AutoTokenizer,T5ForConditionalGeneration, AutoModelForSequenceClassification, AutoModel,AutoModelForSeq2SeqLM\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7806,
     "status": "ok",
     "timestamp": 1749461045725,
     "user": {
      "displayName": "이경민AI융합학과",
      "userId": "11168314652259053945"
     },
     "user_tz": -540
    },
    "id": "KmHL5to5pnmd",
    "outputId": "fab02e2e-1696-4a8e-d45c-51ddaa2fbcd0"
   },
   "outputs": [],
   "source": [
    "# # matplotlib 한글 폰트 설정 (Colab 환경)\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.font_manager as fm\n",
    "# import matplotlib as mpl\n",
    "\n",
    "# # 한글 폰트 설치 (Nanum Gothic 기준)\n",
    "# !sudo apt-get install -y fonts-nanum\n",
    "# !sudo fc-cache -fv\n",
    "# !rm ~/.cache/matplotlib -rf\n",
    "# plt.rc('font', family='NanumBarunGothic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 845,
     "status": "ok",
     "timestamp": 1749461046573,
     "user": {
      "displayName": "이경민AI융합학과",
      "userId": "11168314652259053945"
     },
     "user_tz": -540
    },
    "id": "D6ODScvppkRT",
    "outputId": "16d9e4b3-5711-430a-93d9-bf4fe97454e9"
   },
   "outputs": [],
   "source": [
    "# ▶️ 환경 변수에 넣어 두거나 직접 문자열로 교체\n",
    "MONGODB_URI = (\"mongodb+srv://coque:hoo8176@clusterloa.tdpglbb.mongodb.net/?retryWrites=true&w=majority\")\n",
    "DB_NAME     = \"lostark\"\n",
    "\n",
    "client = MongoClient(MONGODB_URI)\n",
    "db      = client[DB_NAME]\n",
    "post_col = db[\"community_posts\"]\n",
    "daily_col = db[\"daily_summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1749461032651,
     "user": {
      "displayName": "이경민AI융합학과",
      "userId": "11168314652259053945"
     },
     "user_tz": -540
    },
    "id": "Mzkf6zc-6avf",
    "outputId": "e24da8f3-0516-4f11-9774-965d023d6b2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: GPU\n"
     ]
    }
   ],
   "source": [
    "device = 0\n",
    "print(f\"Using device: {'GPU' if device==0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3289,
     "status": "ok",
     "timestamp": 1749461049864,
     "user": {
      "displayName": "이경민AI융합학과",
      "userId": "11168314652259053945"
     },
     "user_tz": -540
    },
    "id": "7s8L75i0rqoi",
    "outputId": "0754c1cf-cef3-403b-f67e-4f6ea8fc831f"
   },
   "outputs": [],
   "source": [
    "# # 2.1 한글 Abstractive 요약 모델\n",
    "# summarizer = pipeline(\n",
    "#     \"summarization\",\n",
    "#     model=\"lcw99/t5-base-korean-text-summary\",\n",
    "#     tokenizer=\"lcw99/t5-base-korean-text-summary\",\n",
    "#     device=device\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ─── 2) digit82/kobart-summarization (KoBART 기반) , posts_embed_summary에 저장하도록 바꾼다.\n",
    "# summarizer_kobart = pipeline(\n",
    "#     \"summarization\",\n",
    "#     model=\"digit82/kobart-summarization\",\n",
    "#     tokenizer=\"digit82/kobart-summarization\",\n",
    "#     device=device,\n",
    "#     torch_dtype=torch.float16,  # FP16 지원 (메모리 절약)\n",
    "#     batch_size=2                # 적절히 조정 가능한 배치 크기\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "#lcw99/t5-base-korean-text-summary 모델에는 daily_summary 저장을 한다.\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# device 설정 (GPU가 있으면 0, 없으면 -1)\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# FP16 + remote_code(필요시) 옵션 포함\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"lcw99/t5-base-korean-text-summary\",\n",
    "    tokenizer=\"lcw99/t5-base-korean-text-summary\",\n",
    "    device=device,\n",
    "    torch_dtype=torch.float16,     # ← 여기에 16비트 지정\n",
    "    trust_remote_code=True,        # lcw99 모델이 remote code일 경우\n",
    "    batch_size=1                   # 메모리 절약용 배치 사이즈\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lcw99/t5-base-korean-text-summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 14998,
     "status": "ok",
     "timestamp": 1749461064864,
     "user": {
      "displayName": "이경민AI융합학과",
      "userId": "11168314652259053945"
     },
     "user_tz": -540
    },
    "id": "DBbV82R8yUy-"
   },
   "outputs": [],
   "source": [
    "# # ─── 3. 전체 “유각” 문서 불러와서 날짜별로 그룹핑 ────────────────────────────\n",
    "# cursor = post_col.find(\n",
    "#     {\n",
    "#         \"keyword\": \"유각\",\n",
    "#         \"date\":    {\"$exists\": True, \"$ne\": None}\n",
    "#     },\n",
    "#     {\"_id\": 0, \"date\": 1, \"text\": 1, \"embedding\": 1}\n",
    "# )\n",
    "\n",
    "# daily = defaultdict(list)\n",
    "# for doc in cursor:\n",
    "#     # \"2025-05-21 19:29\" → \"2025-05-21\"\n",
    "#     day = doc[\"date\"][:10]\n",
    "#     daily[day].append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 원하는 날짜 범위\n",
    "start_date = \"2025-05-01\"\n",
    "end_date = \"2025-05-31\"\n",
    "\n",
    "cursor = post_col.find(\n",
    "    {\n",
    "        \"keyword\": \"보석\",\n",
    "        \"date\": {\n",
    "            \"$gte\": start_date,\n",
    "            \"$lte\": end_date\n",
    "        }\n",
    "    },\n",
    "    {\"_id\": 0, \"date\": 1, \"text\": 1,\"title\":1,\"url\":1, \"embedding\": 1}\n",
    ")\n",
    "\n",
    "daily = defaultdict(list)\n",
    "for doc in cursor:\n",
    "    # \"2025-05-21 19:29\" → \"2025-05-21\"\n",
    "    day = doc[\"date\"][:10]\n",
    "    daily[day].append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1749461064865,
     "user": {
      "displayName": "이경민AI융합학과",
      "userId": "11168314652259053945"
     },
     "user_tz": -540
    },
    "id": "3yx38mZT9rG_"
   },
   "outputs": [],
   "source": [
    "# ─── 토큰 기준 청크 함수 정의 ────────────────────────────────────────────\n",
    "def make_token_chunks(texts, max_tokens=512):\n",
    "    chunks, current, cur_len = [], [], 0\n",
    "    for txt in texts:\n",
    "        toks = tokenizer(txt, add_special_tokens=False)[\"input_ids\"]\n",
    "        if cur_len + len(toks) > max_tokens:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current, cur_len = [], 0\n",
    "        current.append(txt)\n",
    "        cur_len += len(toks)\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZbv1E0vtdhh",
    "outputId": "a77dca56-bf71-484b-eb43-ceec4c35d58b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Your max_length is set to 20, but your input_length is only 19. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 2025-05-01 =====\n",
      "Summary: 나이스단팁은 10겁작이면 뒤로가기하고 주력기 3개는 끼고있는거에서 한단계 위 겁화로 바꿔라 한단계 아래 보석으로 나이스 캐릭들 다 귀속시켜라 3~5프로면 뒤진다고 꿈깨라 유각20장을 꽁으로 준다 나이스단 이득보는만큼 캐릭귀속하면 +@로 추가능력치 줄텐데 나이스단용 보석하고싶으면 캐릭귀속못한대신 +@만큼 보석 레벨 더 올리면 되는거고 유의미한 상승량이면 나이스단이 접느니 헛소릴 처 스마게의 의도는 레이드 출시에 따른 유저들의 스펙업 템포가 안맞아 하향된 것이다.\n",
      "나이스단이 손해보는거 하나도 없고 비나이스단에 비해 귀속옵션으로 인한 공증을 못받을 뿐인데 수치적 손해는 하나도 없고 비나이스단만 더 좋아지는건데 전재학이 왜 욕을 먹는지 이해가 안된다 나이스단은 귀속 이벤트보석 사용하게 해주는 큰 뜻인 이벤트보석 8겁작 1개씩 줄 것이고 이벤트보석 8겁작 1개씩 줄 것이다.\n",
      "1. (sim=0.895) 귀속보석에게 주는 추가공증이 개사기급 거의 보석 1단계 업그레이드급으로 준다 -> 아랫구간 레이드는 모르겠는데, 상위 구간 레이드에선 노귀속 보석인 나이스단 걸러질 가능성이 제법 ...\n",
      "2. (sim=0.894) 나이스단 vs 이벤트단 아님?ㅋㅋㅋ  어짜피 파티 취업은 즐로아나 로펙 점수 볼텐데 귀속 보너스가 ㅈㄴ 크지 않는한 큰 문제는 없어보임  나이스단 : 귀속 안할거면 나이스로 이득 ...\n",
      "3. (sim=0.892) 1줄 요약 : 지금 본캐포함 1680 원대 라인이 제일 현명한 사람들   보석귀속 이 기존보다 더쌔지면 급한대로 보석 한단계 쪼개서 귀속으로 돌려쓰면 되는거임 뭐 귀속보석 해봤자 ...\n",
      "[2025-05-01] 저장 완료\n",
      "\n",
      "===== 2025-05-02 =====\n",
      "Summary: 원정대 6회 제한이 생겼을 때 서포터들은 6캐릭터가 아니였기 때문에 서포터 클래스 6개는 만들어 줘야 하는거 아닌가 한다                                     나이스단 보석이 문제가 아니라 귀속 골드가 더 문제인데 강화 리셋 구조로 인한 성장 동력 상실 상태가 개선이 안됐는데 강제 귀속 시키고 기존 거래 가능 골드를 깎는다면 강제 적금 시키는 것이다 시즌3는 본인들 잘못 덮는 통수가 대부분이고 5~6캐릭 나이스면 이참에 보석값도 떨어진김에 투자해서 9겁작 나이스하면 상관없을거 같다.\n",
      "1. (sim=0.906) 어차핀 공팟은 즐로아컷으로 굴러갈거임  그리고 그걸 나이스단하는 고렙들은 다 알고있을거고  보석을 아낀만큼 쌀을 판 사람들도 있지만 보통 나이스단들은 보석 유각을 돌려쓰면서 레이드...\n",
      "2. (sim=0.903) 그 애들 스팩 보는거 편하게 바꿔준다고도 했고  던담 맹키로 즐로아 보고 받기가 형성 잘되면  나이스단이 누리던건 적은돈으로 고랩보석 돌려쓰기니까 차피 의미 없는거 아님? 귀속 보...\n",
      "3. (sim=0.895) 보석패치 자유로운 셋팅변화를 위해  캐릭 귀속시 겁작 변환및 보석 프리셋화 거래 묶이는거 보상차원에서 ‘약간의’ 기본공증 보너스 라고 말했는데  이걸 나이스단죽이니 보품단죽이니로 ...\n",
      "[2025-05-02] 저장 완료\n",
      "\n",
      "===== 2025-05-03 =====\n",
      "Summary: 부캐 1680 이하 부캐 키우는 나이스 형들은 7겁작 한세트만 더 사라고 하는데 광휘에 달린 기본공격력에 매몰되어 사이클을 앞당기는 쿨타임의 유무를 무시하고 있다   나이스단이 탄생하게 된 이유는 여러가지 스펙업 요소가 많아지는데 세팅비가 부담스러워 보석이라도 돌려쓰기 위해 생겨남 스펙컷이 레이드 적정스펙에 비해 높은 이유는 유저들의 선택으로 인해 스펙컷이 높아짐 나이스단 놈들은 보석 패치 취소를 원하는 게 아니라 보석 패치 취소를 원하는 것이다.\n",
      "나이스단이 화난건 보석의 문제가 아니라 게임에 신뢰가 없어지는 것이다 라방에서 선발대가 상위권이 받는 보상을 신경써주고 40 주차단을 잡는다고 선발대들이 돌아올까 돌아오지도 않는다  나이스단 말고 6캐릭 다른 캐릭을 키우는 사람은 6캐릭 유저들이 나이스단이 될 수 있다.\n",
      "1. (sim=0.907) 본섭, 나이스단 아닌사람들 이 힘드니까 저쪽을 쳐내서 덜꼽게 하겠습니다 ㅋㅋㅋㅋㅋ  끼얏호우 내 보석부담은 아무런 변함이없지만 아무튼 승리했다 환호해 전재학!!  내가 뭘보고 있는...\n",
      "2. (sim=0.904) 여름 유입이라 보석 거지임 나이스 아닌건 주력8겁 나머지7겁작 본캐 2100점 달성해서 이제부터 7겁작 매수해서 60 80소울 귀속 박을거임 한달 주차하면 되는데 뭐가 문제임? 그...\n",
      "3. (sim=0.901) 이번 공지 보고 드는 생각 몇가지  1. 9~10렙 낀 나이스단을 위해 보석 분해 시스템 패치 해주면 좋겠다  2. 본캐만 8겁작 이상 귀속 해놓고 나머진 7겁작 미귀속 돌려 쓰자...\n",
      "[2025-05-03] 저장 완료\n",
      "\n",
      "===== 2025-05-04 =====\n",
      "Summary: 전재학 : 보석의 자율성을 높히겠다고 보석을 귀속화하여 보품단을 처단할 것이며 원정대 나이스단은 불편하지만 쓸 수 있으니 불편하게 쓰세요 불편하면 다 사서 맞추세요 자율 시즌제를 하려면 손해볼게 너무 많아 보석 원대화 한다는 말 나오면 보석값 나락가는거에 겁먹어서 반대하고 있으면서 시즌제 게임을 어떻게 하려고 통보식 패치하면 하루만에 휴짓조각 될 것이다.\n",
      "보석이 아닌 유각 때문에 2~3나이스단이 많은 이유는 유각 공유하는 캐릭터에 눈이 안 가기 때문이다.\n",
      "1. (sim=0.906) 지금 8겁작 ㅡㅡ>  9,10겁작 안 가는 이유? 골드대비 효율이 ㅈ박아서 그돈씨 바로 나옴 전재학이 겁작 변환 공지 나온지 하루도 안 되서 사람들이 생각하는 2교대단?? 생각 안...\n",
      "2. (sim=0.903) 광휘패치가 들어왔을경우 이후의 파급효과는 생각도 안하고  현역입장에서 본인들의 보석가치'만'을 따지는 겁니다  원정대화 시키면 당연히 보석들 쏟아지겠죠  그리고 보석물량 풀렸다고 ...\n",
      "3. (sim=0.900) 로아에서 원정대 구성 한도가 6캐릭으로 막혀있는 순간, 유저는 선택을 해야만 한다. 처음엔 단순하다. “내가 해보고 싶은 캐릭”, 다른 게임에서도 늘 하던 칼쟁이, 활쟁이, 법사,...\n",
      "[2025-05-04] 저장 완료\n",
      "\n",
      "===== 2025-05-05 =====\n",
      "Summary: 라방 때보다 더 좋은 방향으로 수정한다면 맘 돌린 유저의 발걸음을 몇 번이나 돌릴 수 있을 것이다                                          모든 경제는 수요와 공급이고 유각은 니들이 이미 읽어서 수요가 없는데 무지성 공급을 늘렸고 보석은 개나소나 나이스단 쳐 하는데 수요가 있겠냐 보석 가격 올리라고 나이스단 패치 롤백하라는건 대체 뭘 어떻게 하라는  로아 씹망해서 더이상 버스로 쌀먹한다고 생각 하지 말고 보석 싹다 팔아버리고 계정 더 늘려서 생기통 확보 > 다계정 생활 돌려서 생활만 빼먹고 다른 겜 쌀먹하는게 이득이다 보석을 뿌려서 가치를 천천히 전각수준으로 만들고 삭제시켰으면 보석을 뿌려서 가치를 천천히 전각수준으로 만들고 보석을 뿌려서 가치를 천천히 전각수준으로 만들고 삭제시켰으면 한다.\n",
      "1. (sim=0.903) 쿠크유입해서 4년한거같은데   완화 퍼주기 반복하는겜인걸 봐왔고 알고있어서 구 아브때부터 무기1강당 1족발인거보고  선발대 달릴생각 절대안함 ㅇㅇ;; 무족건 너프먹으면 다음 배럭라...\n",
      "2. (sim=0.902) 얘네도 지금 로아 안그래도 쉬러가서 딴겜하러 가는사람 많아서  플레이타임 많이 줄어든거 알고있을텐데  이렇게 나이스단!! 너네 다접어! 라고 생각하진 않았을거야.  라방에서 말도 ...\n",
      "3. (sim=0.901) 하도 돈쓴유저랑 아닌유저랑 차이가 없다고 wwe 걸어서 ^알았다^ 하면서 유각도 악세도 돈쓴 사람 아닌사람 차이나게 해줌  근데 아이템만 그렇게 해줬지 실상 레이드는 유저층 다 섞...\n",
      "[2025-05-05] 저장 완료\n",
      "\n",
      "===== 2025-05-06 =====\n",
      "Summary: 보석은 두 번의 파동이 있는데 사태가 잠잠해지면 보석 값이 일시적으로 오름 그리고 두 번째 파동은 10겁 보상 8렙 보석이 들어올 때 다시 한번 보석 값을 박는다  카제가 아크라시아 박살내기도 전에 유각, 보석, 로크 단 세단어로 선발대, 나이스단, 후발주자, 쌀먹까지 싹 다 잡아먹음 선발대는 감가에 박탈감으로 대거 이탈  더퍼스트 이벤트가 있는 컨텐츠가 나오는데 스펙업 현금 비중이 존나 커서 유각 시스템을 삭제하면 반발할 것이 뻔해 유각을 안읽어 놓으면 트라이 조차 못할테니 가격을 박살내서 트라이 인원을 늘림 로아온으로 다시 유저들의 마음을 돌릴 수 있을 가능성이 얼마나 될지 모르겠다.\n",
      "1. (sim=0.901) 6나이스단이 접어서 보석 팜  경매장에 보석 11개 올라옴  6나이스단이 욕하면서 보석 삼  본캐 광휘로 쓰고 5나이스 보석 11개 구매 보석안사고 똑같이 돌림  다계정나이스 보석...\n",
      "2. (sim=0.894) 2막 아브렐슈드 나왔을떄 뭐라도 시도해야했음 악세값 유각값 보석값 미친듯이 올라가고 ( 상단일 1연마 35~45만골드 , 아드레날린 장당 가장 많이 올랐을떈 55~60라인 그 후 ...\n",
      "3. (sim=0.894) 일단 보석얘기를 제일 길게 할거같은데  심심하면 보셈    1.  직변권 5개 기간제로 지급 - 여홀나는 선택못함 , 슈모익 지정캐릭터도 못함  - 어차피 직변권 낸다고 매출감소도...\n",
      "[2025-05-06] 저장 완료\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     ps \u001b[38;5;241m=\u001b[39m \u001b[43msummarizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# OOM 발생 시 Extractive 백업\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     ps \u001b[38;5;241m=\u001b[39m summarize(c, word_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:298\u001b[0m, in \u001b[0;36mSummarizationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    275\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m          ids of the summary.\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:186\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    158\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[0;32m    191\u001b[0m     ):\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\transformers\\pipelines\\base.py:1431\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1424\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1425\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         )\n\u001b[0;32m   1429\u001b[0m     )\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\transformers\\pipelines\\base.py:1438\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1437\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1438\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1439\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\transformers\\pipelines\\base.py:1338\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1336\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1337\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1338\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1339\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:215\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[0;32m    213\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m--> 215\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    216\u001b[0m out_b \u001b[38;5;241m=\u001b[39m output_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\transformers\\generation\\utils.py:2616\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2609\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2610\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2611\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2612\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2614\u001b[0m     )\n\u001b[0;32m   2615\u001b[0m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2616\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[0;32m   2617\u001b[0m         input_ids,\n\u001b[0;32m   2618\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2619\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2620\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2621\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2622\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2623\u001b[0m     )\n\u001b[0;32m   2625\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2626\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2627\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2628\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2629\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2635\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2636\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\transformers\\generation\\utils.py:4030\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   4027\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   4028\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m-> 4030\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   4032\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   4033\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   4034\u001b[0m     model_outputs,\n\u001b[0;32m   4035\u001b[0m     model_kwargs,\n\u001b[0;32m   4036\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   4037\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1811\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1808\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[1;32m-> 1811\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1812\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1814\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1820\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1821\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1822\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1825\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1827\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1829\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1124\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1107\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1108\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[0;32m   1109\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1121\u001b[0m         cache_position,\n\u001b[0;32m   1122\u001b[0m     )\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1124\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:729\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    726\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m attention_outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m    728\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[1;32m--> 729\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:343\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m    342\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 343\u001b[0m     forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDenseReluDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwarded_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(forwarded_states)\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:311\u001b[0m, in \u001b[0;36mT5DenseGatedActDense.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m--> 311\u001b[0m     hidden_gelu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwi_0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m     hidden_linear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwi_1(hidden_states)\n\u001b[0;32m    313\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_gelu \u001b[38;5;241m*\u001b[39m hidden_linear\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\lee\\.conda\\envs\\loa309\\lib\\site-packages\\transformers\\activations.py:47\u001b[0m, in \u001b[0;36mNewGELUActivation.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.044715\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ─── 4. 일자별 요약 & 대표 게시물 선별 & 저장 ───────────────────────────────\n",
    "for day, docs in sorted(daily.items()):\n",
    "    texts = [\n",
    "        f\"{d['title']}\\n{d['text']}\"\n",
    "        for d in docs\n",
    "    ]\n",
    "    embs  = np.vstack([d[\"embedding\"] for d in docs]).astype(np.float32)\n",
    "\n",
    "    # 4.1 토큰 청크링 + 계층적 Abstractive 요약\n",
    "    chunks = make_token_chunks(texts, max_tokens=512)\n",
    "    partial_summaries = []\n",
    "    for c in chunks:\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            ps = summarizer(\n",
    "                c,\n",
    "                max_new_tokens=80,\n",
    "                min_length=20,\n",
    "                do_sample=False\n",
    "            )[0][\"summary_text\"]\n",
    "        except RuntimeError:\n",
    "            # OOM 발생 시 Extractive 백업\n",
    "            ps = summarize(c, word_count=80)\n",
    "        partial_summaries.append(ps)\n",
    "\n",
    "    # 최종 요약을 다시 요약\n",
    "    final_input = \" \".join(partial_summaries)\n",
    "    try:\n",
    "        summary = summarizer(final_input, max_length=100, min_length=30)[0][\"summary_text\"]\n",
    "    except RuntimeError:\n",
    "        summary = summarize(final_input, word_count=100)\n",
    "\n",
    "    # # 4.2 대표 게시물 선별 (KMeans)\n",
    "    # k = min(5, len(docs))\n",
    "    # km = KMeans(n_clusters=k, random_state=42).fit(embs)\n",
    "    # representatives = []\n",
    "    # for center in km.cluster_centers_:\n",
    "    #     sims = cosine_similarity([center], embs)[0]\n",
    "    #     idx  = int(np.argmax(sims))\n",
    "    #     rep = docs[idx]\n",
    "    #     representatives.append({\n",
    "    #         \"title\":       rep.get(\"title\"),\n",
    "    #         \"url\":         rep.get(\"url\"),\n",
    "    #         \"text\":        rep.get(\"text\"),\n",
    "    #         \"similarity\":  float(sims[idx])\n",
    "    #     })\n",
    "\n",
    "    # ─── 4.2 대표 게시물 선별 (Centroid 방식, 중복 방지) ────────────────────────────────\n",
    "    k = 3  # 뽑을 대표 게시물 수\n",
    "    centroid   = embs.mean(axis=0)                        # 1. 평균 벡터(centroid)\n",
    "    sims       = cosine_similarity([centroid], embs)[0]   # 2. centroid ↔ 각 문서 유사도\n",
    "    sorted_idx = np.argsort(sims)[::-1]                   # 3. 유사도 내림차순 인덱스\n",
    "\n",
    "    representatives = []\n",
    "    seen_urls       = set()\n",
    "\n",
    "    for idx in sorted_idx:\n",
    "        rep = docs[idx]\n",
    "        url = rep.get(\"url\")\n",
    "        # 이미 선택된 URL이면 건너뛰기\n",
    "        if url in seen_urls:\n",
    "            continue\n",
    "        seen_urls.add(url)\n",
    "\n",
    "        representatives.append({\n",
    "            \"title\":       rep.get(\"title\"),\n",
    "            \"url\":         url,\n",
    "            \"text\":        rep.get(\"text\"),\n",
    "            \"similarity\":  float(sims[idx])\n",
    "        })\n",
    "        # k개 다 모았으면 종료\n",
    "        if len(representatives) >= k:\n",
    "            break\n",
    "\n",
    "    # 4.3 출력 (저장 전 확인)\n",
    "    print(f\"\\n===== {day} =====\")\n",
    "    print(\"Summary:\", summary)\n",
    "    for i, rep in enumerate(representatives, 1):\n",
    "        print(f\"{i}. (sim={rep['similarity']:.3f}) {rep['text'][:100]}...\")\n",
    "\n",
    "    # 4.4 MongoDB에 저장\n",
    "    daily_col.update_one(\n",
    "        {\"date\": day, \"keyword\": \"보석\"},\n",
    "        {\"$set\": {\n",
    "            \"date\":            day,\n",
    "            \"count\":           len(docs),\n",
    "            \"summary\":         summary,\n",
    "            \"representatives\": representatives,\n",
    "            \"keyword\":         \"보석\"\n",
    "        }},\n",
    "        upsert=True\n",
    "    )\n",
    "    print(f\"[{day}] 저장 완료\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNjBicCegHwR7gyLjWd5HEq",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "loa309",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
